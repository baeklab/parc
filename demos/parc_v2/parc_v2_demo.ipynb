{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from keras.layers import *\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxilary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_unit(feat_dim, kernel_size, x_in):\n",
    "    # conv = Conv2D(feats, kernel, padding=\"same\")\n",
    "    res = keras.Sequential([\n",
    "        Conv2D(feat_dim,\n",
    "               kernel_size, padding=\"same\",\n",
    "               kernel_initializer='he_uniform',\n",
    "               bias_initializer='he_uniform'),\n",
    "        ReLU(),\n",
    "        Conv2D(feat_dim,\n",
    "               kernel_size,\n",
    "               padding=\"same\",\n",
    "               kernel_initializer='he_uniform',\n",
    "            bias_initializer='he_uniform')\n",
    "    ])\n",
    "    return ReLU()(x_in + res(x_in))\n",
    "\n",
    "def resnet_block(x_in, feat_dim, kernel_size, reps, pooling = True):\n",
    "    # Stage 2\n",
    "    conv1 = Conv2D(feat_dim,\n",
    "                   kernel_size,\n",
    "                   padding=\"same\",\n",
    "                   kernel_initializer='he_uniform',\n",
    "                   bias_initializer='he_uniform')(x_in)\n",
    "    relu1 = ReLU()(conv1)\n",
    "    conv2 = Conv2D(feat_dim,\n",
    "                   kernel_size,\n",
    "                   padding=\"same\",\n",
    "                   kernel_initializer='he_uniform',\n",
    "                   bias_initializer='he_uniform')(relu1)\n",
    "    x = ReLU()(conv2)\n",
    "    for _ in range(reps):\n",
    "        x = resnet_unit(feat_dim,kernel_size,x)\n",
    "    if pooling == True:\n",
    "        x = MaxPooling2D(2,2)(x)\n",
    "        return x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_unit(feat_dim, kernel_size, x):\n",
    "    x = Conv2D(feat_dim, \n",
    "               kernel_size, \n",
    "               activation = LeakyReLU(0.2), \n",
    "               padding=\"same\", \n",
    "               kernel_initializer='he_uniform', \n",
    "               bias_initializer='he_uniform')(x)\n",
    "    x = Conv2D(feat_dim,\n",
    "               1,\n",
    "               activation = LeakyReLU(0.2),\n",
    "               padding=\"same\",\n",
    "               kernel_initializer='he_uniform',\n",
    "               bias_initializer='he_uniform')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block_down(x, feat_dim, reps, kernel_size, mode = 'normal'):\n",
    "    if mode == 'down':\n",
    "        x = MaxPooling2D(2,2)(x)\n",
    "    for _ in range(reps):\n",
    "        x = conv_unit(feat_dim, \n",
    "                      kernel_size,\n",
    "                      x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def conv_block_up_w_concat(x, x1, feat_dim, reps, kernel_size, mode = 'normal'):\n",
    "    if mode == 'up':\n",
    "        x = UpSampling2D((2,2))(x)\n",
    "    \n",
    "    x = Concatenate()([x,x1])\n",
    "    for _ in range(reps):\n",
    "        x = conv_unit(feat_dim,\n",
    "                      kernel_size,\n",
    "                      x)\n",
    "    return x\n",
    "\n",
    "def conv_block_up_wo_concat(x, feat_dim, reps, kernel_size, mode = 'normal'):\n",
    "    if mode == 'up':\n",
    "        x = UpSampling2D((2,2))(x)\n",
    "    for _ in range(reps):\n",
    "        x = conv_unit(feat_dim,\n",
    "                      kernel_size,\n",
    "                      x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPADE(layers.Layer):\n",
    "    def __init__(self, filters, epsilon=1e-5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        self.conv = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\",kernel_initializer='he_uniform',\n",
    "               bias_initializer='he_uniform')\n",
    "        self.conv_gamma = layers.Conv2D(filters, 3, padding=\"same\",kernel_initializer='he_uniform',\n",
    "               bias_initializer='he_uniform')\n",
    "        self.conv_beta = layers.Conv2D(filters, 3, padding=\"same\",kernel_initializer='he_uniform',\n",
    "               bias_initializer='he_uniform')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.resize_shape = input_shape[1:3]\n",
    "        # print(self.resize_shape)\n",
    "\n",
    "    def call(self, input_tensor, raw_mask):\n",
    "        mask = tf.image.resize(raw_mask, self.resize_shape, method=\"nearest\")\n",
    "        x = self.conv(mask)    \n",
    "\n",
    "        gamma = self.conv_gamma(x)\n",
    "        beta = self.conv_beta(x)\n",
    "        mean, var = tf.nn.moments(input_tensor, axes=(0, 1, 2), keepdims=True)\n",
    "        std = tf.sqrt(var + self.epsilon)\n",
    "\n",
    "        normalized = (input_tensor - mean) / std\n",
    "        output = gamma * normalized + beta\n",
    "\n",
    "        return output\n",
    "\n",
    "def spade_generator_unit(x, mask, feats_in, kernel, upsampling = True):\n",
    "    x = GaussianNoise(0.05)(x)\n",
    "    # SPADE & conv\n",
    "    spade1 = SPADE(feats_in)(x, mask)\n",
    "    output = Conv2D(feats_in,kernel, padding='same', activation= LeakyReLU(0.2),kernel_initializer='he_uniform',\n",
    "               bias_initializer='he_uniform')(spade1)\n",
    "    if upsampling == True:\n",
    "        output = UpSampling2D(size = (2,2))(output)\n",
    "        return output\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(layers.Layer):\n",
    "    def __init__(self, filters, epsilon=1e-5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        self.dense = layers.Dense(filters,\n",
    "                                  activation = 'relu',\n",
    "                                  kernel_initializer='he_uniform',\n",
    "                                  bias_initializer='he_uniform')\n",
    "        self.dense_gamma = layers.Dense(filters,\n",
    "                                        kernel_initializer='he_uniform',\n",
    "                                        bias_initializer='he_uniform')\n",
    "        self.dense_beta = layers.Dense(filters,\n",
    "                                       kernel_initializer='he_uniform',\n",
    "                                       bias_initializer='he_uniform')\n",
    "\n",
    "    def call(self, input_tensor, style_vector):\n",
    "        x = self.dense(style_vector)\n",
    "        gamma = self.dense_gamma(x)\n",
    "        beta = self.dense_beta(x)\n",
    "        #Normalize x[0]\n",
    "        mean, var = tf.nn.moments(input_tensor, axes=(0, 1, 2), keepdims=True)\n",
    "        std = tf.sqrt(var + self.epsilon)\n",
    "        normalized = (input_tensor - mean) / std\n",
    "        y = (x[0] - mean) / std\n",
    "\n",
    "        #Reshape gamma and beta\n",
    "        pool_shape = [-1, 1, 1, y.shape[-1]]\n",
    "        gamma = tf.reshape(gamma, pool_shape) + 1.0\n",
    "        beta = tf.reshape(beta, pool_shape)\n",
    "\n",
    "        return gamma * normalized + beta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_unet(n_out_features = 128, n_base_features = 64, n_channel = 5):\n",
    "    inputs = keras.Input(shape = (256,512,n_channel))\n",
    "\n",
    "    conv1 = conv_block_down(inputs,\n",
    "                            feat_dim = n_base_features,\n",
    "                            reps = 1,\n",
    "                            kernel_size = 3)\n",
    "    conv2 = conv_block_down(conv1,\n",
    "                            feat_dim = n_base_features*2,\n",
    "                            reps = 1,\n",
    "                            kernel_size = 3,\n",
    "                            mode = 'down')\n",
    "    conv3 = conv_block_down(conv2,\n",
    "                            feat_dim = n_base_features*4,\n",
    "                            reps = 1,\n",
    "                            kernel_size = 3,\n",
    "                            mode = 'down')\n",
    "    conv4 = conv_block_down(conv3,\n",
    "                            feat_dim = n_base_features*8,\n",
    "                            reps = 1,\n",
    "                            kernel_size = 3,\n",
    "                            mode = 'down')\n",
    "    conv5 = conv_block_down(conv4,\n",
    "                            feat_dim = n_base_features*16,\n",
    "                            reps = 1,\n",
    "                            kernel_size = 3,\n",
    "                            mode = 'down')\n",
    "    conv6 = conv_block_up_wo_concat(conv5,\n",
    "                                    feat_dim = n_base_features*8,\n",
    "                                    reps = 1,\n",
    "                                    kernel_size = 3,\n",
    "                                    mode = 'up')\n",
    "    \n",
    "    conv7 = conv_block_up_w_concat(conv6, conv3,\n",
    "                                    feat_dim = n_base_features*4,\n",
    "                                    reps = 1,\n",
    "                                    kernel_size = 3,\n",
    "                                    mode = 'up')\n",
    "    \n",
    "    conv8 = conv_block_up_wo_concat(conv7,\n",
    "                                    feat_dim = n_base_features*2,\n",
    "                                    reps = 1,\n",
    "                                    kernel_size = 3,\n",
    "                                    mode = 'up')\n",
    "    \n",
    "    conv9 = conv_block_up_w_concat(conv8, conv1,\n",
    "                                    feat_dim = n_out_features,\n",
    "                                    reps = 1,\n",
    "                                    kernel_size = 3,\n",
    "                                    mode = 'up')\n",
    "\n",
    "    feature_out = conv_block_up_wo_concat(conv9,\n",
    "                                    feat_dim = n_out_features,\n",
    "                                    reps = 1,\n",
    "                                    kernel_size = 1,\n",
    "                                    mode = 'normal')\n",
    "    unet = keras.Model(inputs, feature_out)\n",
    "    return unet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping and reconstruction CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_and_recon_cnn(n_base_features = 64, input_shape = (256,512,128), n_mask_channel = 1, output_channel = 1 ):\n",
    "    inputs = keras.Input(shape = input_shape)\n",
    "    inputs_2 = keras.Input(shape = (256,512,n_mask_channel))\n",
    "    spade1 = spade_generator_unit(inputs,\n",
    "                                  inputs_2,\n",
    "                                  128,\n",
    "                                  1,\n",
    "                                  upsampling = False)\n",
    "    conv1 = resnet_block(spade1,\n",
    "                         feat_dim = n_base_features,\n",
    "                         kernel_size = 1,\n",
    "                         reps = 2,\n",
    "                         pooling = False)\n",
    "    spade2 = spade_generator_unit(conv1,\n",
    "                                  inputs_2,\n",
    "                                  n_base_features,\n",
    "                                  1,\n",
    "                                  upsampling = False)\n",
    "    conv2 = resnet_block(spade2,\n",
    "                         feat_dim = n_base_features*2,\n",
    "                         kernel_size = 1,\n",
    "                         reps = 2,\n",
    "                         pooling = False)\n",
    "    \n",
    "    spade3 = spade_generator_unit(conv2,\n",
    "                                  inputs_2,\n",
    "                                  n_base_features*2,\n",
    "                                  1,\n",
    "                                  upsampling = False)\n",
    "    conv3 = resnet_block(spade3,\n",
    "                         feat_dim = n_base_features*4,\n",
    "                         kernel_size = 1,\n",
    "                         reps = 2,\n",
    "                         pooling = False)\n",
    "    conv_out = Conv2D(output_channel,1,padding='same')(conv3)\n",
    "    mapping_resnet = keras.Model([inputs,inputs_2], conv_out)\n",
    "    return mapping_resnet\n",
    "# mapping_and_recon_cnn().summary()\n",
    "# PARCv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Advection(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, state_variable, velocity_field):\n",
    "        dy, dx = tf.image.image_gradients(state_variable)\n",
    "        spatial_deriv = tf.concat([dy,dx],axis = -1)\n",
    "        advect = tf.reduce_sum(tf.multiply(spatial_deriv,velocity_field),axis = -1, keepdims=True)\n",
    "        return advect\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiator CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 256, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 256, 512, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 256, 512, 5)  0           ['input_10[0][0]',               \n",
      "                                                                  'input_11[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 256, 512, 1)  0          ['input_10[0][0]']               \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 256, 512, 1)  0          ['input_10[0][0]']               \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 256, 512, 1)  0          ['input_10[0][0]']               \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 256, 512, 12  15070144    ['concatenate_2[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " advection (Advection)          (None, 256, 512, 1)  0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'input_11[0][0]']              \n",
      "                                                                                                  \n",
      " advection_1 (Advection)        (None, 256, 512, 1)  0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'input_11[0][0]']               \n",
      "                                                                                                  \n",
      " advection_2 (Advection)        (None, 256, 512, 1)  0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'input_11[0][0]']               \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 256, 512, 1)  1186753     ['model[0][0]',                  \n",
      "                                                                  'advection[0][0]']              \n",
      "                                                                                                  \n",
      " model_2 (Functional)           (None, 256, 512, 1)  1186753     ['model[0][0]',                  \n",
      "                                                                  'advection_1[0][0]']            \n",
      "                                                                                                  \n",
      " model_3 (Functional)           (None, 256, 512, 1)  1186753     ['model[0][0]',                  \n",
      "                                                                  'advection_2[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256, 512, 3)  0           ['advection[0][0]',              \n",
      "                                                                  'advection_1[0][0]',            \n",
      "                                                                  'advection_2[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 256, 512, 3)  0           ['model_1[0][0]',                \n",
      "                                                                  'model_2[0][0]',                \n",
      "                                                                  'model_3[0][0]']                \n",
      "                                                                                                  \n",
      " model_4 (Functional)           (None, 256, 512, 2)  1192770     ['model[0][0]',                  \n",
      "                                                                  'concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,823,173\n",
      "Trainable params: 19,823,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def differentiator(n_state_var = 3):\n",
    "    feature_extraction = feature_extraction_unet(n_channel=n_state_var+2)\n",
    "        # Mapping and recon CNN\n",
    "    mapping_and_recon = []\n",
    "    advection = []\n",
    "    for _ in range(n_state_var):\n",
    "        mapping_and_recon.append(mapping_and_recon_cnn())\n",
    "        advection.append(Advection())\n",
    "\n",
    "    velocity_mapping_and_recon = mapping_and_recon_cnn(n_mask_channel = n_state_var, output_channel=2)\n",
    "    \n",
    "    init_state_var = keras.layers.Input(shape = (256,512,n_state_var))\n",
    "    velocity_field = keras.layers.Input(shape = (256,512,2))\n",
    "\n",
    "    concat1 = keras.layers.concatenate([init_state_var,velocity_field],axis = -1)\n",
    "    dynamic_feature = feature_extraction(concat1)\n",
    "    advec = []\n",
    "    state_var_dot = []\n",
    "    \n",
    "    for i in range(n_state_var): \n",
    "        advec.append(advection[i](init_state_var[:,:,:,i:i+1],velocity_field))\n",
    "        state_var_dot.append(mapping_and_recon[i]([dynamic_feature,advec[i]]))\n",
    "\n",
    "    advec_concat = keras.layers.concatenate(advec,axis = -1)\n",
    "    state_var_dot_concat = keras.layers.concatenate(state_var_dot,axis = -1)\n",
    "    \n",
    "    velocity_dot = velocity_mapping_and_recon([dynamic_feature,advec_concat])\n",
    "    differentiator = keras.Model([init_state_var, velocity_field], [state_var_dot_concat, velocity_dot])\n",
    "    return differentiator\n",
    "    \n",
    "differentiator(n_state_var=3).summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrator_cnn(n_base_features = 128, n_output = 1):\n",
    "    inputs = keras.Input(shape = (256,512,n_output))\n",
    "    conv1 = resnet_block(inputs,\n",
    "                         feat_dim = n_base_features,\n",
    "                         kernel_size = 1,\n",
    "                         reps = 2,\n",
    "                         pooling = False)\n",
    "    conv2 = resnet_block(conv1,\n",
    "                         feat_dim = n_base_features*2,\n",
    "                         kernel_size = 1,\n",
    "                         reps = 2,\n",
    "                         pooling = False)\n",
    "    conv3 = resnet_block(conv2,\n",
    "                         feat_dim = n_base_features*4,\n",
    "                         kernel_size = 1,\n",
    "                         reps = 2,\n",
    "                         pooling = False)\n",
    "    conv_out = Conv2D(n_output,1,padding='same')(conv3)\n",
    "    integrator_resnet = keras.Model(inputs, conv_out)\n",
    "    return integrator_resnet\n",
    "# integrator_cnn().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 256, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 256, 512, 1)  0          ['input_16[0][0]']               \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 256, 512, 1)  0          ['input_16[0][0]']               \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 256, 512, 1)  0          ['input_16[0][0]']               \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " model_6 (Functional)           (None, 256, 512, 1)  1890177     ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " model_7 (Functional)           (None, 256, 512, 1)  1890177     ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " model_8 (Functional)           (None, 256, 512, 1)  1890177     ['tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 256, 512, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 256, 512, 3)  0           ['model_6[0][0]',                \n",
      "                                                                  'model_7[0][0]',                \n",
      "                                                                  'model_8[0][0]']                \n",
      "                                                                                                  \n",
      " model_9 (Functional)           (None, 256, 512, 2)  1890818     ['input_17[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,561,349\n",
      "Trainable params: 7,561,349\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def integrator(n_state_var = 3):\n",
    "    state_integrators = []\n",
    "    for _ in range(n_state_var):\n",
    "        state_integrators.append(integrator_cnn())\n",
    "\n",
    "    velocity_integrator = integrator_cnn(n_output=2)\n",
    "\n",
    "    state_var_dot = keras.layers.Input(shape = (256,512,n_state_var))\n",
    "    velocity_dot = keras.layers.Input(shape = (256,512,2))\n",
    "\n",
    "    state_var_next = []\n",
    "        \n",
    "    for i in range(n_state_var): \n",
    "        state_var_next.append(state_integrators[i](state_var_dot[:,:,:,i:i+1]))\n",
    "\n",
    "    state_var_next = keras.layers.concatenate(state_var_next, axis=-1)\n",
    "    velocity_next = velocity_integrator(velocity_dot)\n",
    "    integrator = keras.Model([state_var_dot, velocity_dot], [state_var_next, velocity_next])\n",
    "    return integrator\n",
    "integrator(n_state_var = 3).summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARCv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"par_cv2_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_54 (InputLayer)          [(None, 256, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_55 (InputLayer)          [(None, 256, 512, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " model_27 (Functional)          [(None, 256, 512, 3  19823173    ['input_54[0][0]',               \n",
      "                                ),                                'input_55[0][0]',               \n",
      "                                 (None, 256, 512, 2               'add_10[0][0]',                 \n",
      "                                )]                                'add_11[0][0]',                 \n",
      "                                                                  'add_12[0][0]',                 \n",
      "                                                                  'add_13[0][0]',                 \n",
      "                                                                  'add_14[0][0]',                 \n",
      "                                                                  'add_15[0][0]',                 \n",
      "                                                                  'add_16[0][0]',                 \n",
      "                                                                  'add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " model_32 (Functional)          [(None, 256, 512, 3  7561349     ['model_27[0][0]',               \n",
      "                                ),                                'model_27[0][1]',               \n",
      "                                 (None, 256, 512, 2               'model_27[1][0]',               \n",
      "                                )]                                'model_27[1][1]',               \n",
      "                                                                  'model_27[2][0]',               \n",
      "                                                                  'model_27[2][1]',               \n",
      "                                                                  'model_27[3][0]',               \n",
      "                                                                  'model_27[3][1]',               \n",
      "                                                                  'model_27[4][0]',               \n",
      "                                                                  'model_27[4][1]']               \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 256, 512, 3)  0           ['input_54[0][0]',               \n",
      "                                                                  'model_32[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 256, 512, 2)  0           ['input_55[0][0]',               \n",
      "                                                                  'model_32[0][1]']               \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 256, 512, 3)  0           ['add_10[0][0]',                 \n",
      "                                                                  'model_32[1][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 256, 512, 2)  0           ['add_11[0][0]',                 \n",
      "                                                                  'model_32[1][1]']               \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 256, 512, 3)  0           ['add_12[0][0]',                 \n",
      "                                                                  'model_32[2][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 256, 512, 2)  0           ['add_13[0][0]',                 \n",
      "                                                                  'model_32[2][1]']               \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 256, 512, 3)  0           ['add_14[0][0]',                 \n",
      "                                                                  'model_32[3][0]']               \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 256, 512, 2)  0           ['add_15[0][0]',                 \n",
      "                                                                  'model_32[3][1]']               \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 256, 512, 3)  0           ['add_16[0][0]',                 \n",
      "                                                                  'model_32[4][0]']               \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 256, 512, 2)  0           ['add_17[0][0]',                 \n",
      "                                                                  'model_32[4][1]']               \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 256, 512, 15  0           ['add_10[0][0]',                 \n",
      "                                )                                 'add_12[0][0]',                 \n",
      "                                                                  'add_14[0][0]',                 \n",
      "                                                                  'add_16[0][0]',                 \n",
      "                                                                  'add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 256, 512, 10  0           ['add_11[0][0]',                 \n",
      "                                )                                 'add_13[0][0]',                 \n",
      "                                                                  'add_15[0][0]',                 \n",
      "                                                                  'add_17[0][0]',                 \n",
      "                                                                  'add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 256, 512, 15  0           ['model_27[0][0]',               \n",
      "                                )                                 'model_27[1][0]',               \n",
      "                                                                  'model_27[2][0]',               \n",
      "                                                                  'model_27[3][0]',               \n",
      "                                                                  'model_27[4][0]']               \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 256, 512, 10  0           ['model_27[0][1]',               \n",
      "                                )                                 'model_27[1][1]',               \n",
      "                                                                  'model_27[2][1]',               \n",
      "                                                                  'model_27[3][1]',               \n",
      "                                                                  'model_27[4][1]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,384,522\n",
      "Trainable params: 27,384,522\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class PARCv2(keras.Model):\n",
    "    def __init__(self, n_state_var, n_time_step, **kwargs):\n",
    "        super(PARCv2, self).__init__(**kwargs)\n",
    "        self.n_state_var = n_state_var\n",
    "        self.n_time_step = n_time_step\n",
    "        self.differentiator = differentiator(n_state_var=self.n_state_var)\n",
    "        self.integrator = integrator(n_state_var=self.n_state_var)\n",
    "\n",
    "        self.init_state_var = keras.layers.Input(shape = (256,512,self.n_state_var))\n",
    "        self.velocity_field = keras.layers.Input(shape = (256,512,2))\n",
    "        self.out = self.call([self.init_state_var, self.velocity_field])\n",
    "\n",
    "        super(PARCv2, self).__init__(\n",
    "            inputs=[self.init_state_var, self.velocity_field], outputs=self.out, **kwargs\n",
    "        )\n",
    "\n",
    "        # loss define\n",
    "        self.differentitator_loss_tracker = keras.metrics.Mean(name=\"differentiator_loss\")\n",
    "        self.state_var_loss_tracker = keras.metrics.Mean(name=\"state_var_loss_tracker\")\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "\n",
    "    def build(self):\n",
    "        self._is_graph_network = True\n",
    "        self._init_graph_network(\n",
    "            inputs=[self.init_state_var, self.velocity_field], outputs=self.out\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "        self.total_loss_tracker,\n",
    "        self.differentitator_loss_tracker,\n",
    "        self.state_var_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        state_var_init = inputs[0]\n",
    "        velocity_init = inputs[1]\n",
    "\n",
    "        state_var_all = []\n",
    "        velocity_all = []\n",
    "        state_var_dot_all = []\n",
    "        velocity_dot_all = []\n",
    "        state_var_current = state_var_init\n",
    "        velocity_current = velocity_init\n",
    "        for _ in range(self.n_time_step):\n",
    "            state_var_dot, velocity_dot = self.differentiator([state_var_current, velocity_current])\n",
    "            delta_state_var, delta_velocity = self.integrator([state_var_dot, velocity_dot])  \n",
    "\n",
    "            state_var_current = keras.layers.add([state_var_current, delta_state_var])\n",
    "            velocity_current = keras.layers.add([velocity_current, delta_velocity])\n",
    "\n",
    "            state_var_dot_all.append(state_var_dot)\n",
    "            velocity_dot_all.append(velocity_dot)\n",
    "            state_var_all.append(state_var_current)\n",
    "            velocity_all.append(velocity_current)\n",
    "        \n",
    "        state_var_dot_all = keras.layers.concatenate(state_var_dot_all, axis = -1)\n",
    "        state_var_all = keras.layers.concatenate(state_var_all, axis = -1)\n",
    "        velocity_dot_all = keras.layers.concatenate(velocity_dot_all, axis = -1)\n",
    "        velocity_all = keras.layers.concatenate(velocity_all, axis = -1)\n",
    "\n",
    "        return state_var_all, velocity_all, state_var_dot_all, velocity_dot_all\n",
    "\n",
    "    def train_step(self, data):\n",
    "        state_var_init = data[0][0]\n",
    "        velocity_init = data[0][1]\n",
    "\n",
    "        state_var_gt = data[1][0]\n",
    "        velocity_gt = data[1][1]\n",
    "        state_var_dot_gt = data[1][2]\n",
    "        velocity_dot_gt = data[1][3]\n",
    "\n",
    "        state_loss = 0\n",
    "        differentiator_loss = 0\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # AE loss\n",
    "            state_var_current = state_var_init\n",
    "            velocity_current = velocity_init\n",
    "            for ts in range(self.n_time_step):\n",
    "                state_var_dot, velocity_dot = self.differentiator([state_var_current, velocity_current])\n",
    "                delta_state_var, delta_velocity = self.integrator([state_var_dot, velocity_dot])  \n",
    "\n",
    "                state_var_current = keras.layers.add([state_var_current, delta_state_var])\n",
    "                velocity_current = keras.layers.add([velocity_current, delta_velocity])\n",
    "\n",
    "                state_range = (ts*self.n_state_var, ts*self.n_state_var+self.n_state_var-1)\n",
    "                vel_range = (ts*2, ts*2+1)\n",
    "                \n",
    "                differentiator_loss += tf.keras.losses.MeanSquaredError()(state_var_dot,state_var_dot_gt[:,:,:,state_range[0]:state_range[1]]) + \\\n",
    "                                        tf.keras.losses.MeanSquaredError()(velocity_dot,velocity_dot_gt[:,:,:,vel_range[0]:vel_range[1]])\n",
    "                state_loss += tf.keras.losses.MeanSquaredError()(state_var_current,state_var_gt[:,:,:,:,state_range[0]:state_range[1]]) + \\\n",
    "                                    tf.keras.losses.MeanSquaredError()(velocity_current,velocity_gt[:,:,:,vel_range[0]:vel_range[1]])\n",
    "\n",
    "            total_loss =  differentiator_loss + state_loss\n",
    "        \n",
    "        self.differentitator_loss_tracker.update_state(differentiator_loss)\n",
    "        self.state_var_loss_tracker.update_state(state_loss) \n",
    "        self.total_loss_tracker.update_state(total_loss) \n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": self.total_loss_tracker.result(),\n",
    "            \"differentiator_loss\": self.differentitator_loss_tracker.result(),\n",
    "            \"state_var_loss_tracker\": self.state_var_loss_tracker.result(),\n",
    "        }\n",
    "parc = PARCv2(n_state_var=3,n_time_step=5)\n",
    "parc.build()\n",
    "parc.compile()\n",
    "parc.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Data processing: \n",
    "# Step 1: read data\n",
    "# Step 2: chop data into sequences (1-ts length sq, 2-ts length sq, 3-ts length sq, etc.)\n",
    "\n",
    "def clipping_raw_data_to_small_sequence(sequence_length = 2, n_state_var = 3):\n",
    "    state_seq_whole = []\n",
    "    vel_seq_whole = []\n",
    "    for i in range (200):\n",
    "        file_path = 'C:/Users/cdy9xh/Work/01.Research/01.PIML/parc-meta/myenv/data/data_processed/coupled_field_with_vel/void_' + str(i) +'.npy'\n",
    "        if os.path.exists(file_path):\n",
    "            raw_data_in = np.load(file_path)\n",
    "            data_shape = np.shape(raw_data_in)\n",
    "            pad_val_y = 0\n",
    "            pad_val_x = 0\n",
    "            if data_shape[0] < 512:\n",
    "                pad_val_y = abs(data_shape[0]-512)\n",
    "            if data_shape[1] < 1024:\n",
    "                pad_val_x = abs(data_shape[1]-1024)\n",
    "                # print(pad_val_x)\n",
    "            npad = ((pad_val_y, 0), (0, pad_val_x), (0, 0))\n",
    "\n",
    "            raw_data_in = np.pad(raw_data_in, pad_width=npad,mode = 'edge')    \n",
    "            raw_data_in = np.expand_dims(raw_data_in,axis = 0)\n",
    "            data_shape = np.shape(raw_data_in)\n",
    "\n",
    "            # print('Void ', str(i))\n",
    "            # print(data_shape)\n",
    "\n",
    "            no_of_ts = np.uint8(data_shape[-1]/(n_state_var+2))\n",
    "            state_seq_case = []\n",
    "            vel_seq_case = []\n",
    "            \n",
    "            for j in range(no_of_ts-sequence_length):\n",
    "                state_seq = []\n",
    "                vel_seq = []\n",
    "\n",
    "                for k in range(sequence_length):\n",
    "                    state_clip_range = ((j+k)*(n_state_var+2),(j+k)*(n_state_var+2)+n_state_var)\n",
    "                    vel_clip_range = ((j+k)*(n_state_var+2)+n_state_var,(j+k)*(n_state_var+2)+n_state_var+2)\n",
    "\n",
    "                    state_seq.append(raw_data_in[:,:,:,state_clip_range[0]:state_clip_range[1]])\n",
    "                    vel_seq.append(raw_data_in[:,:,:,vel_clip_range[0]:vel_clip_range[1]])\n",
    "\n",
    "                state_seq_case.append(np.concatenate(state_seq, axis = -1))\n",
    "                vel_seq_case.append(np.concatenate(vel_seq,axis = -1))\n",
    "\n",
    "            state_seq_case = np.concatenate(state_seq_case,axis = 0)\n",
    "            vel_seq_case = np.concatenate(vel_seq_case,axis = 0)\n",
    "\n",
    "            # print(state_seq_case.shape)\n",
    "\n",
    "            state_seq_whole.append(state_seq_case)\n",
    "            vel_seq_whole.append(vel_seq_case)\n",
    "\n",
    "\n",
    "    state_seq_whole = np.concatenate(state_seq_whole, axis = 0)\n",
    "    vel_seq_whole = np.concatenate(vel_seq_whole, axis = 0)\n",
    "    return state_seq_whole, vel_seq_whole\n",
    "\n",
    "state_seq_whole, vel_seq_whole = clipping_raw_data_to_small_sequence(sequence_length = 2, n_state_var = 3)\n",
    "# Normalize data\n",
    "\n",
    "# Step 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3196, 512, 1024, 6)\n",
      "(3196, 512, 1024, 4)\n"
     ]
    }
   ],
   "source": [
    "print(state_seq_whole.shape)\n",
    "print(vel_seq_whole.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (3196):\n",
    "    iter_max = np.amax(state_seq_whole[i,:,:,0::3])    \n",
    "    # iter_min = np.amin(vel_seq_whole[i,:,:,0::2])    \n",
    "    # if iter_max_u > max_val_u: max_val_u = iter_max_u\n",
    "    # if iter_min_u < min_val_u: min_val_u = iter_min_u\n",
    "    print(iter_max)\n",
    "\n",
    "    # if iter_max_v > max_val_v: max_val_v = iter_max_v\n",
    "    # if iter_min_v < min_val_v: min_val_v = iter_min_v\n",
    "\n",
    "    # if iter_max > max_val: max_val = iter_max\n",
    "\n",
    "# max_val = -1e12\n",
    "# min_val = 1e-4\n",
    "# state_seq_whole_test = np.array(state_seq_whole)\n",
    "# for i in range (20):\n",
    "#     iter_max = np.amax(state_seq_whole[i,:,:,1::3])    \n",
    "#     if iter_max > max_val: max_val = iter_max\n",
    "\n",
    "# Normalizaion\n",
    "# state_seq_whole[:,:,:,1::3] = ((state_seq_whole[:,:,:,1::3] - min_val)/(max_val - min_val))*2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def clip_raw_data_to_small_sequence(sequence_length=2, n_state_var=3):\n",
    "    state_seq_whole = []\n",
    "    vel_seq_whole = []\n",
    "    for i in range(200):\n",
    "        file_path = os.path.join('C:', os.sep, 'Users', 'cdy9xh', 'Work', '01.Research', '01.PIML', 'parc-meta', 'myenv', 'data', 'data_processed', 'coupled_field_with_vel', f'void_{i}.npy')\n",
    "        if os.path.exists(file_path):\n",
    "            raw_data = np.load(file_path)\n",
    "            data_shape = raw_data.shape\n",
    "            npad = ((0, abs(data_shape[0] - 512)), (0, abs(data_shape[1] - 1024)), (0, 0))\n",
    "\n",
    "            raw_data = np.pad(raw_data, pad_width=npad, mode='edge')\n",
    "            raw_data = np.expand_dims(raw_data, axis=0)\n",
    "            data_shape = raw_data.shape\n",
    "\n",
    "            num_time_steps = data_shape[-1] // (n_state_var + 2)\n",
    "            state_seq_case = [np.concatenate([raw_data[:, :, :, (j + k) * (n_state_var + 2):\\\n",
    "                                                    (j + k) * (n_state_var + 2) + n_state_var] \\\n",
    "                                                    for k in range(sequence_length)], axis=-1) \\\n",
    "                                                    for j in range(num_time_steps - sequence_length)]\n",
    "            vel_seq_case = [np.concatenate([raw_data[:, :, :, (j + k) * (n_state_var + 2) +  n_state_var :\\\n",
    "                                                    (j + k) * (n_state_var + 2) + n_state_var + 2] \\\n",
    "                                                    for k in range(sequence_length)], axis=-1) \\\n",
    "                                                    for j in range(num_time_steps - sequence_length)]\n",
    "\n",
    "            state_seq_whole.extend(state_seq_case)\n",
    "            vel_seq_whole.extend(vel_seq_case)\n",
    "\n",
    "    state_seq_whole = np.concatenate(state_seq_whole, axis=0)\n",
    "    vel_seq_whole = np.concatenate(vel_seq_whole, axis=0)\n",
    "\n",
    "    return state_seq_whole, vel_seq_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_seq_whole[:,:,:,0::3] = ((state_seq_whole[:,:,:,0::3] - 300)/(5000 - 300))*2 - 1\n",
    "# state_seq_whole[:,:,:,1::3] = ((state_seq_whole[:,:,:,1::3] - 1e-4)/(30e9 - 1e-4))*2 - 1\n",
    "vel_seq_whole[:,:,:,0::2] = ((vel_seq_whole[:,:,:,0::2] + 2875))/(9275 + 2875)*2 - 1\n",
    "vel_seq_whole[:,:,:,1::2] = ((vel_seq_whole[:,:,:,1::2] + 6865)/(6650 + 6865))*2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('state_var_sq_len_1.npy',state_seq_whole)\n",
    "np.save('vel_sq_len_1.npy',state_seq_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13fb90f8c70>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(vel_seq_whole[100,:,:,1]))#, vmin= -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_seq_whole[:,:,:,1::3] = (state_seq_whole[:,:,:,1::3] + 1)/2*(50e9 - 1e-4) + 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "state_seq_whole_train = skimage.measure.block_reduce(np.squeeze(state_seq_whole) , (1,2,2,1),np.max)\n",
    "vel_seq_whole_train = skimage.measure.block_reduce(np.squeeze(vel_seq_whole) , (1,2,2,1),np.max)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- The training is based on the idea of stacking delta_t blocks\n",
    "- PARCv2 will be trained with an incremental for one time step at a time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cdy9xh\\Documents\\GitHub\\parc\\demos\\parc_v2\\parc_v2_demo.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cdy9xh/Documents/GitHub/parc/demos/parc_v2/parc_v2_demo.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# parc = PARCv2(n_state_var = 3, n_time_step = 1)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cdy9xh/Documents/GitHub/parc/demos/parc_v2/parc_v2_demo.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# parc.build()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cdy9xh/Documents/GitHub/parc/demos/parc_v2/parc_v2_demo.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# parc.summary()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cdy9xh/Documents/GitHub/parc/demos/parc_v2/parc_v2_demo.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# parc.compile()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cdy9xh/Documents/GitHub/parc/demos/parc_v2/parc_v2_demo.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m parc\u001b[39m.\u001b[39mcompile(optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, beta_1 \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m, beta_2 \u001b[39m=\u001b[39m \u001b[39m0.999\u001b[39m),metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cdy9xh/Documents/GitHub/parc/demos/parc_v2/parc_v2_demo.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m history \u001b[39m=\u001b[39m parc\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m[state_seq_whole[:,:,:,:\u001b[39m3\u001b[39;49m],vel_seq_whole[:,:,:,:\u001b[39m2\u001b[39;49m]], y \u001b[39m=\u001b[39;49m[state_seq_whole[:,:,:,\u001b[39m3\u001b[39;49m:],vel_seq_whole[:,:,:,\u001b[39m2\u001b[39;49m:]],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cdy9xh/Documents/GitHub/parc/demos/parc_v2/parc_v2_demo.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, epochs \u001b[39m=\u001b[39;49m \u001b[39m300\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\cdy9xh\\Work\\01.Research\\01.PIML\\parc\\parc-venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\cdy9xh\\Work\\01.Research\\01.PIML\\parc\\parc-venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# parc = PARCv2(n_state_var = 3, n_time_step = 1)\n",
    "# parc.build()\n",
    "# parc.summary()\n",
    "# parc.compile()\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(lr=0.0001, beta_1 = 0.9, beta_2 = 0.999),metrics=['mse'])\n",
    "history = parc.fit(x=[state_seq_whole_train[:,:,:,:3],vel_seq_whole_train[:,:,:,:2]], y =[state_seq_whole_train[:,:,:,3:],vel_seq_whole_train[:,:,:,2:]],\n",
    "                    batch_size=1, epochs = 300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
